[{"categories":["hugo"],"content":"博客迁移至hugo。 记录下迁移至hugo的折腾路。 待续。。。。 ","date":"2023-04-18","objectID":"/posts/change-to-hugo/:0:0","tags":["hugo"],"title":"博客迁移至hugo","uri":"/posts/change-to-hugo/"},{"categories":["编程"],"content":"实现 spark dataframe/dataset 根据mysql表唯一键实现有则更新，无则插入功能。 基于 spark2.4.3 scala2.11.8 ","date":"2023-04-18","objectID":"/posts/spark-mysql-upsert/:0:0","tags":["spark","大数据"],"title":"spark 实现 mysql upsert","uri":"/posts/spark-mysql-upsert/"},{"categories":["编程"],"content":"工具类 DataFrameWriterEnhance package com.xxx.utils import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan import org.apache.spark.sql.execution.SQLExecution import org.apache.spark.sql.execution.datasources.DataSource import org.apache.spark.sql.execution.datasources.jdbc.{JDBCOptions, JdbcOptionsInWrite, JdbcRelationProvider, JdbcUtils} import org.apache.spark.sql.jdbc.{JdbcDialect, JdbcDialects} import org.apache.spark.sql.sources.BaseRelation import org.apache.spark.sql.types.StructType import org.apache.spark.sql._ import java.sql.Connection object DataFrameWriterEnhance { implicit class DataFrameWriterMysqlUpdateEnhance(writer: DataFrameWriter[Row]) { def update(): Unit = { val extraOptionsField = writer.getClass.getDeclaredField(\"org$apache$spark$sql$DataFrameWriter$$extraOptions\") val dfField = writer.getClass.getDeclaredField(\"df\") val sourceField = writer.getClass.getDeclaredField(\"source\") val partitioningColumnsField = writer.getClass.getDeclaredField(\"partitioningColumns\") extraOptionsField.setAccessible(true) dfField.setAccessible(true) sourceField.setAccessible(true) partitioningColumnsField.setAccessible(true) val extraOptions = extraOptionsField.get(writer).asInstanceOf[scala.collection.Map[String, String]] val df = dfField.get(writer).asInstanceOf[DataFrame] val partitioningColumns = partitioningColumnsField.get(writer).asInstanceOf[Option[Seq[String]]] val logicalPlanField = df.getClass.getDeclaredField(\"logicalPlan\") logicalPlanField.setAccessible(true) var logicalPlan = logicalPlanField.get(df).asInstanceOf[LogicalPlan] val session = df.sparkSession val dataSource = DataSource( sparkSession = session, className = s\"${DataFrameWriterEnhance.getClass.getName}MysqlUpdateRelationProvider\", partitionColumns = partitioningColumns.getOrElse(Nil), options = extraOptions.toMap) logicalPlan = dataSource.planForWriting(SaveMode.Append, logicalPlan) val qe = session.sessionState.executePlan(logicalPlan) SQLExecution.withNewExecutionId(session, qe)(qe.toRdd) } } class MysqlUpdateRelationProvider extends JdbcRelationProvider { override def createRelation(sqlContext: SQLContext, mode: SaveMode, parameters: Map[String, String], df: DataFrame): BaseRelation = { val options = new JdbcOptionsInWrite(parameters) val isCaseSensitive = sqlContext.sparkSession.sessionState.conf.caseSensitiveAnalysis val conn = JdbcUtils.createConnectionFactory(options)() try { val tableExists = JdbcUtils.tableExists(conn, options) if (tableExists) { mode match { case SaveMode.Overwrite =\u003e if (options.isTruncate \u0026\u0026 JdbcUtils.isCascadingTruncateTable(options.url).contains(false)) { // In this case, we should truncate table and then load. JdbcUtils.truncateTable(conn, options) val tableSchema = JdbcUtils.getSchemaOption(conn, options) updateTable(df, tableSchema, isCaseSensitive, options) } else { // Otherwise, do not truncate the table, instead drop and recreate it JdbcUtils.dropTable(conn, options.table, options) JdbcUtils.createTable(conn, df, options) updateTable(df, Some(df.schema), isCaseSensitive, options) } case SaveMode.Append =\u003e val tableSchema = JdbcUtils.getSchemaOption(conn, options) updateTable(df, tableSchema, isCaseSensitive, options) case SaveMode.ErrorIfExists =\u003e throw new Exception( s\"Table or view '${options.table}' already exists. \" + s\"SaveMode: ErrorIfExists.\") case SaveMode.Ignore =\u003e // With `SaveMode.Ignore` mode, if table already exists, the save operation is expected // to not save the contents of the DataFrame and to not change the existing data. // Therefore, it is okay to do nothing here and then just return the relation below. } } else { JdbcUtils.createTable(conn, df, options) updateTable(df, Some(df.schema), isCaseSensitive, options) } } finally { conn.close() } createRelation(sqlContext, parameters) } def updateTable(df: DataFrame, tableSchema: Option[StructType], isCaseSensitive: Boolean, options: JdbcOptionsInWrite): Unit = { val url = options.url val table = options.table val dialect = JdbcDialects.get(url)","date":"2023-04-18","objectID":"/posts/spark-mysql-upsert/:1:0","tags":["spark","大数据"],"title":"spark 实现 mysql upsert","uri":"/posts/spark-mysql-upsert/"},{"categories":["编程"],"content":"工具类 MysqlUtils package com.xxx.utils import com.xxx.utils.DataFrameWriterEnhance.DataFrameWriterMysqlUpdateEnhance import org.apache.spark.sql.functions.col import org.apache.spark.sql.types.{NullType, ShortType} import org.apache.spark.sql.{DataFrame, SaveMode, SparkSession} object MysqlUtils { def upsert(rawDF: DataFrame, database: String, tableName: String)(implicit spark: SparkSession): Unit = { var df = rawDF for (elem \u003c- df.schema.fields) { if (elem.dataType == NullType) { df = df.withColumn(elem.name, col(elem.name).cast(ShortType)) } } df.write .format(\"jdbc\") .mode(SaveMode.Append) .option(\"driver\", \"com.mysql.jdbc.Driver\") .option(\"url\", spark.conf.get(s\"spark.job.mysql.${database}.url\")) .option(\"user\", spark.conf.get(s\"spark.job.mysql.${database}.username\")) .option(\"password\", spark.conf.get(s\"spark.job.mysql.${database}.password\")) .option(\"dbtable\", tableName) .option(\"useSSL\", \"false\") .option(\"showSql\", \"false\") .option(\"numPartitions\", \"1\") .update() } } ","date":"2023-04-18","objectID":"/posts/spark-mysql-upsert/:2:0","tags":["spark","大数据"],"title":"spark 实现 mysql upsert","uri":"/posts/spark-mysql-upsert/"},{"categories":["编程"],"content":"使用 spark启动脚本加入mysql配置 spark-submit \\ --master yarn \\ --deploy-mode cluster \\ --executor-memory 3G \\ --num-executors 5 \\ --executor-cores 4 \\ --driver-memory 3G \\ --conf spark.job.mysql.test.url=${jdbc_url} \\ --conf spark.job.mysql.test.username=${jdbc_username} \\ --conf spark.job.mysql.test.password=${jdbc_password} \\ 使用范例 import utils.MysqlUtils object TestMysqlUpsert { def main(args: Array[String]): Unit = { implicit val spark = SparkSession.builder().enableHiveSupport().getOrCreate() import spark.implicits._ val database = \"test\" val arr = Array((1,11,\"name1\",11111),(2,22,\"name2\",22222)) val df = spark.sparkContext.parallelize(arr) .toDF(\"key_one\", \"key_two\", \"val_one\", \"val_two\") MysqlUtils.upsert(df, database, \"test_unique_key\") spark.close() } } test_unique_key表结构 CREATE TABLE `test_unique_key` ( `key_one` int(11) NOT NULL DEFAULT '0', `key_two` int(11) NOT NULL DEFAULT '0', `val_one` varchar(50) DEFAULT NULL, `val_two` int(11) NOT NULL DEFAULT '0', UNIQUE KEY `uk` (`key_one`,`key_two`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='test'; ","date":"2023-04-18","objectID":"/posts/spark-mysql-upsert/:3:0","tags":["spark","大数据"],"title":"spark 实现 mysql upsert","uri":"/posts/spark-mysql-upsert/"},{"categories":["编程"],"content":"参考 csdn_Spark Upsert写入Mysql(scala增强) 无需依赖 ","date":"2023-04-18","objectID":"/posts/spark-mysql-upsert/:4:0","tags":["spark","大数据"],"title":"spark 实现 mysql upsert","uri":"/posts/spark-mysql-upsert/"},{"categories":["编程"],"content":"访问某接口拉取数据，接口需要频次控制，经调研，com.google.common.util.concurrent.RateLimiter可轻易实现。 ","date":"2023-04-18","objectID":"/posts/java-rate-limit-google-guava/:0:0","tags":["java"],"title":"java 频次控制","uri":"/posts/java-rate-limit-google-guava/"},{"categories":["编程"],"content":"1. Maven \u003cdependency\u003e \u003cgroupId\u003ecom.google.guava\u003c/groupId\u003e \u003cartifactId\u003eguava\u003c/artifactId\u003e \u003cversion\u003e31.1-jre\u003c/version\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003ecn.hutool\u003c/groupId\u003e \u003cartifactId\u003ehutool-all\u003c/artifactId\u003e \u003cversion\u003e5.8.4\u003c/version\u003e \u003c/dependency\u003e ","date":"2023-04-18","objectID":"/posts/java-rate-limit-google-guava/:1:0","tags":["java"],"title":"java 频次控制","uri":"/posts/java-rate-limit-google-guava/"},{"categories":["编程"],"content":"2. 测试代码 import cn.hutool.http.HttpRequest; import com.google.common.util.concurrent.RateLimiter; import java.util.List; import java.util.Map; import java.util.concurrent.TimeUnit; public class RateLimiterTest { private final RateLimiter rateLimiter = RateLimiter.create(20,1000, TimeUnit.MILLISECONDS); private RateLimiterTest() { } private static final class InstanceHolder { static final RateLimiterTest instance = new RateLimiterTest(); } public static RateLimiterTest getInstance() { return InstanceHolder.instance; } public String runHttpGet(String url, Map\u003cString, List\u003cString\u003e\u003e header) { rateLimiter.acquire(1); return HttpRequest .get(url) .header(header) .execute() .body(); } } import java.util.*; import java.util.concurrent.*; public class TestRate { public static void main(String[] args) throws InterruptedException, ExecutionException { String url = \"https://xxxxx\"; Map\u003cString, List\u003cString\u003e\u003e header = new HashMap\u003c\u003e(); header.put(\"Content-type\", Collections.singletonList(\"application/json\")); header.put(\"Access-Token\", Collections.singletonList(\"xxxxx\")); List\u003cCallable\u003cString\u003e\u003e tasks = new ArrayList\u003c\u003e(); for (int i = 0; i \u003c 60; i++) { Callable\u003cString\u003e callable = () -\u003e { RateLimiterTest instance = RateLimiterTest.getInstance(); return instance.runHttpGet(url, header); }; tasks.add(callable); } ExecutorService exec = Executors.newFixedThreadPool(60); List\u003cFuture\u003cString\u003e\u003e futures = exec.invokeAll(tasks); for (Future\u003cString\u003e future : futures) { String s = future.get(); System.out.println(s); } exec.shutdown(); } } ","date":"2023-04-18","objectID":"/posts/java-rate-limit-google-guava/:2:0","tags":["java"],"title":"java 频次控制","uri":"/posts/java-rate-limit-google-guava/"},{"categories":["编程"],"content":" import easyocr import torch gpu_is_available = torch.cuda.is_available() reader = easyocr.Reader(['ch_sim', 'en'], gpu=gpu_is_available) ocr_data = reader.readtext('/image/path', detail=0, text_threshold=0.7, low_text=0.1, width_ths=0.3) detail = 0 仅输出识别到的文字 text_threshold=0.7 文本置信阈值，值越小，文本块内被识别为文字的可能性越高 low_text=0.1 值越小，图片内容被识别为文本块的可能性就更高 width_ths=0.3 控制横向图片内容是否会被识别为一个或多个文本块，值越小被识别成多个文本块的可能性就更高 待续… 参考： 官方文档 Python使用EasyOCR库对行程码图片进行OCR文字识别介绍与实践 ","date":"2023-04-18","objectID":"/posts/python3-easyocr-simple/:0:0","tags":["python"],"title":"python3 easyocr 简单使用识别参数","uri":"/posts/python3-easyocr-simple/"},{"categories":["编程"],"content":"presto自带unbase64函数某些时候会报错，所以想要自定义一个unbase64函数。 presto自带unbase64函数，用法如下。 FROM_UTF8(from_base64(nickname)) 但是有些字符会报错。 Query failed (#20220720_091551_00087_mkhun): Illegal base64 character -1a ","date":"2023-04-18","objectID":"/posts/presto-udf-simple/:0:0","tags":["java","大数据"],"title":"presto 自定义函数简述","uri":"/posts/presto-udf-simple/"},{"categories":["编程"],"content":"idea新建maven项目 pom文件如下： \u003cdependencies\u003e \u003cdependency\u003e \u003cgroupId\u003ecom.facebook.presto\u003c/groupId\u003e \u003cartifactId\u003epresto-spi\u003c/artifactId\u003e \u003cversion\u003e0.272\u003c/version\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003ecom.google.guava\u003c/groupId\u003e \u003cartifactId\u003eguava\u003c/artifactId\u003e \u003cversion\u003e18.0\u003c/version\u003e \u003c/dependency\u003e \u003c/dependencies\u003e \u003cbuild\u003e \u003cplugins\u003e \u003cplugin\u003e \u003cgroupId\u003eorg.apache.maven.plugins\u003c/groupId\u003e \u003cartifactId\u003emaven-jar-plugin\u003c/artifactId\u003e \u003cconfiguration\u003e \u003cclassesDirectory\u003etarget/classes/\u003c/classesDirectory\u003e \u003carchive\u003e \u003cmanifest\u003e \u003cmainClass\u003ecom.alibaba.dubbo.container.Main\u003c/mainClass\u003e \u003c!-- 打包时 MANIFEST.MF文件不记录的时间戳版本 --\u003e \u003cuseUniqueVersions\u003efalse\u003c/useUniqueVersions\u003e \u003caddClasspath\u003etrue\u003c/addClasspath\u003e \u003cclasspathPrefix\u003elib/\u003c/classpathPrefix\u003e \u003c/manifest\u003e \u003cmanifestEntries\u003e \u003cClass-Path\u003e.\u003c/Class-Path\u003e \u003c/manifestEntries\u003e \u003c/archive\u003e \u003c/configuration\u003e \u003c/plugin\u003e \u003cplugin\u003e \u003cgroupId\u003eorg.apache.maven.plugins\u003c/groupId\u003e \u003cartifactId\u003emaven-dependency-plugin\u003c/artifactId\u003e \u003cexecutions\u003e \u003cexecution\u003e \u003cid\u003ecopy-dependencies\u003c/id\u003e \u003cphase\u003epackage\u003c/phase\u003e \u003cgoals\u003e \u003cgoal\u003ecopy-dependencies\u003c/goal\u003e \u003c/goals\u003e \u003cconfiguration\u003e \u003ctype\u003ejar\u003c/type\u003e \u003cincludeTypes\u003ejar\u003c/includeTypes\u003e \u003coutputDirectory\u003e ${project.build.directory}/lib \u003c/outputDirectory\u003e \u003c/configuration\u003e \u003c/execution\u003e \u003c/executions\u003e \u003c/plugin\u003e \u003cplugin\u003e \u003cgroupId\u003ecom.facebook.presto\u003c/groupId\u003e \u003cartifactId\u003epresto-maven-plugin\u003c/artifactId\u003e \u003cversion\u003e0.3\u003c/version\u003e \u003cextensions\u003etrue\u003c/extensions\u003e \u003c/plugin\u003e \u003c/plugins\u003e \u003c/build\u003e ","date":"2023-04-18","objectID":"/posts/presto-udf-simple/:1:0","tags":["java","大数据"],"title":"presto 自定义函数简述","uri":"/posts/presto-udf-simple/"},{"categories":["编程"],"content":"function开发 package base64; import com.facebook.presto.common.type.StandardTypes; import com.facebook.presto.spi.function.Description; import com.facebook.presto.spi.function.ScalarFunction; import com.facebook.presto.spi.function.SqlType; import io.airlift.slice.Slice; import io.airlift.slice.Slices; public class UnBase64Function { @ScalarFunction(\"unBase64\") // 固定参数，表示函数名的意思，也就我们在使用Presto的时候用的函数名 @Description(\"base64解密\") // 函数的注释 @SqlType(StandardTypes.VARCHAR) // 表示数据类型 public static Slice unBase64(@SqlType(StandardTypes.VARCHAR) Slice input) { // 解码 String res = \"\"; try { byte[] decodeBytes = java.util.Base64.getDecoder().decode(input.toStringUtf8()); res = new String(decodeBytes, \"utf-8\"); } catch (Exception e) { } return Slices.utf8Slice(res); } } ","date":"2023-04-18","objectID":"/posts/presto-udf-simple/:2:0","tags":["java","大数据"],"title":"presto 自定义函数简述","uri":"/posts/presto-udf-simple/"},{"categories":["编程"],"content":"plugin开发 import java.util.Set; import base64.Base64Function; import base64.UnBase64Function; import com.facebook.presto.spi.Plugin; import com.google.common.collect.ImmutableSet; public class PrestoUdfPlugin implements Plugin { @Override public Set\u003cClass\u003c?\u003e\u003e getFunctions() { return ImmutableSet.\u003cClass\u003c?\u003e\u003ebuilder() // 添加插件class .add(UnBase64Function.class) .build(); } } ","date":"2023-04-18","objectID":"/posts/presto-udf-simple/:3:0","tags":["java","大数据"],"title":"presto 自定义函数简述","uri":"/posts/presto-udf-simple/"},{"categories":["编程"],"content":"加载plugin 在src/main/resources下创建目录META-INF，再在META-INF目录下创建services目录，注意下图项目结构中META-INF是父目录 services是子目录，只是idea合并显示了，不是说文件夹名里面有点 “.” 。然后创建文件com.facebook.presto.spi.Plugin，文件内容为plugin的全类名。 例如本例：PrestoUdfPlugin ","date":"2023-04-18","objectID":"/posts/presto-udf-simple/:4:0","tags":["java","大数据"],"title":"presto 自定义函数简述","uri":"/posts/presto-udf-simple/"},{"categories":["编程"],"content":"上线 maven打包。 在所有presto节点服务器的presto安装目录${PRESTO_HOME}/plugin下新建一个my_plugin目录。 上传jar包，及相关依赖包（在lib目录下）到所有服务器新建的my_plugin目录下。 重启presto： ${PRESTO_HOME}/bin/launcher restart。 项目结构 结构 参考：【presto】方法二：presto实现自定义UDF函数 ","date":"2023-04-18","objectID":"/posts/presto-udf-simple/:5:0","tags":["java","大数据"],"title":"presto 自定义函数简述","uri":"/posts/presto-udf-simple/"},{"categories":["编程"],"content":" from urllib import parse import pandas as pd from sqlalchemy import create_engine db_info = {'user': 'test', 'password': parse.quote_plus('test'), 'host': 'xxxxxx.mysql.rds.aliyuncs.com', 'port': 3306, 'database': 'test' } engine = create_engine( 'mysql+pymysql://%(user)s:%(password)s@%(host)s:%(port)d/%(database)s?charset=utf8' % db_info, encoding='utf-8') def mysql_replace_into(table, conn, keys, data_iter): from sqlalchemy.dialects.mysql import insert data = [dict(zip(keys, row)) for row in data_iter] stmt = insert(table.table).values(data) update_stmt = stmt.on_duplicate_key_update(**dict(zip(stmt.inserted.keys(), stmt.inserted.values()))) conn.execute(update_stmt) def write_database(df, table_name): df.to_sql(table_name, con=engine, index=False, if_exists='append', method=mysql_replace_into) if __name__ == '__main__': data = [ {\"app_id\": 1, \"app_secret\": \"11111122\", \"game_id\": 11}, {\"app_id\": 2, \"app_secret\": \"22222233\", \"game_id\": 22}, {\"app_id\": 3, \"app_secret\": \"33333344\", \"game_id\": 33} ] detailDF = pd.DataFrame(data) write_database(detailDF, \"ad_app_ext\") ","date":"2023-04-18","objectID":"/posts/python3-pandas-mysql-upsert/:0:0","tags":["python"],"title":"python3 pandas 实现mysql upsert操作（唯一键更新）","uri":"/posts/python3-pandas-mysql-upsert/"},{"categories":["编程"],"content":" sqoop export \\ --update-key unique_index_columns \\ --update-mode allowinsert 问题描述： 用上述模式 sqoop 导入数据更新 mysql 数据，无论导入的数据与mysql里数据相比有没有更新，mysql表的 AUTO_INCREMENT 的PRIMARY KEY 例如 （id int(11) NOT NULL AUTO_INCREMENT COMMENT ‘自增主键’） 实际都是增加了的，当有新唯一主键数据插入时，id会是累加后的开始，会变的很大。 解决： sqoop源码层级暂时没看原因。。。 可以从业务流程上解决下这个问题，保证导入的数据都是新增或者更新的，不会有与mysql一样的数据，可以一定程度上减少 id 的增长。 ","date":"2023-04-18","objectID":"/posts/sqoop-mysql-update-auto-increment/:0:0","tags":["sqoop","大数据"],"title":"sqoop mysql update AUTO_INCREMENT 自增主键重复增长问题","uri":"/posts/sqoop-mysql-update-auto-increment/"},{"categories":["编程"],"content":"脚本 #! /bin/bash source /etc/profile set +o posix # to enable process substitution when not running on bash scala_file=$1 shift 1 arguments=$@ ##### scala 文件后加 sys.exit spark-shell --master yarn \\ --executor-cores 5 \\ --num-executors 4 \\ --executor-memory 8g \\ --driver-memory 3g \\ --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \\ --conf spark.default.parallelism=400 \\ --conf spark.sql.shuffle.partitions=400 \\ --conf spark.shuffle.io.maxRetries=10 \\ --conf spark.shuffle.io.retryWait=20s \\ --conf spark.yarn.executor.memoryOverhead=4096 \\ --conf spark.network.timeout=300s \\ --name sparkshell_scala \\ -i \u003c(echo 'val args = \"'$arguments'\".split(\"\\\\s+\")' ; cat $scala_file) 简单scala文件示例 val path = args(0) spark.read.parquet(path).where(\"app_id = 77701\").repartition(1).write.parquet(s\"${path}_new\") sys.exit 如果不需要传参，可简单使用 spark-shell \u003c test.scala ","date":"2023-04-18","objectID":"/posts/bash-spark-shell-scala/:0:0","tags":["大数据","spark","scala"],"title":"脚本执行spark-shell scala文件退出","uri":"/posts/bash-spark-shell-scala/"},{"categories":["编程"],"content":"背景： 线上kudu 集群版本为1.11.0版本, spark 使用 kudu-spark2_2.11-1.7.0.jar， 为了使用新版本中的 val wo = new KuduWriteOptions(ignoreNull = true) 特性，升级至 kudu-spark2_2.11-1.11.0.jar 版本，但是报错。 java.io.InvalidClassException: org.apache.kudu.spark.kudu.KuduContext; local class incompatible: stream classdesc serialVersionUID = xxxxxxxx, local class serialVersionUID = xxxxxxx111 在https://issues.apache.org/jira/browse/KUDU-2898 中说这个已经修复，但是修复的版本是kudu 1.12.0，升级线上kudu版本这个暂时不考虑，只好自己修改源码，重新编译 kudu-spark2_2.11-1.11.0.jar 1.解压 kudu-spark2_2.11-1.11.0.jar ，将解压出的所有文件夹移动至一个空文件夹，例如tmp 2.在idea中新建maven项目，配置好 scala 等环境 3.解压 kudu-spark2_2.11-1.11.0-sources.jar，将 其中 org 文件夹 复制 至 maven中刚刚建好的项目中（org/apache/kudu/spark/kudu） 4.修改文件 KuduContext.scala 、 KuduRDD.scala， 增加或修改 @SerialVersionUID(xxxxxxxxxxL) ，xxxxxxxxxx改为报错信息中的local class serialVersionUID 5：OperationType.scala文件也需修改，和 上面的serialVersionUID不同，走完整个流程运行会报这个文件的serialVersionUID错误，改成错误信息中的local class serialVersionUID 6.将 kudu-spark2_2.11-1.11.0.pom 中的 dependencies 粘贴至 maven 中项目的pom文件中，还需额外增加以下依赖： \u003cdependency\u003e \u003cgroupId\u003eorg.apache.kudu\u003c/groupId\u003e \u003cartifactId\u003ekudu-client\u003c/artifactId\u003e \u003cversion\u003e1.11.0\u003c/version\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.apache.yetus\u003c/groupId\u003e \u003cartifactId\u003eaudience-annotations\u003c/artifactId\u003e \u003cversion\u003e0.13.0\u003c/version\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.hdrhistogram\u003c/groupId\u003e \u003cartifactId\u003eHdrHistogram\u003c/artifactId\u003e \u003cversion\u003e2.1.12\u003c/version\u003e \u003c/dependency\u003e 7.idea中将maven项目打包 8.将idea中打好的包解压，将其中 org/apache/kudu/spark/kudu 下的文件覆盖至 tmp 中 org/apache/kudu/spark/kudu 9.将 tmp 文件夹下的所有目录打包成新的jar包 参考链接： https://community.cloudera.com/t5/Support-Questions/KUDU-need-rebuild-post-upgrade-of-CDH-cluster/td-p/92885 https://issues.apache.org/jira/browse/KUDU-2898 ","date":"2023-04-18","objectID":"/posts/kudu-spark-invalid-class-exception/:0:0","tags":["大数据","kudu"],"title":"kudu-spark KuduContext  java.io.InvalidClassException 解决","uri":"/posts/kudu-spark-invalid-class-exception/"},{"categories":["编程"],"content":"项目用到phoenix，使用了这个jar包phoenix-client-4.14.1-HBase-1.4.jar，这个jar包导致的jar包冲突很多，一番摸索，解决了，解决如下。 先jar命令解压jar包，然后删除以下内容。然后在jar命令打成jar包。 rm -r javax/ rm -r com/jayway/ rm -r org/apache/phoenix/shaded/org/apache/thrift rm -r com/sun/jersey/ ","date":"2023-04-18","objectID":"/posts/phoenix-client-hbase-jar/:0:0","tags":["大数据","hbase"],"title":"phoenix-client-4.14.1-HBase-1.4.jar jar包冲突解决","uri":"/posts/phoenix-client-hbase-jar/"},{"categories":null,"content":"自娱自乐，随便写写，难得自在! 从事大数据开发工作，欢迎交流。 ","date":"2023-04-18","objectID":"/about/:0:0","tags":null,"title":"关于","uri":"/about/"},{"categories":["编程"],"content":"当 hive 表的分区字段 是 date 类型时，用如下方式读取会发生报错。 val targetDay = \"2020-08-20\" spark.read.table(tableName) .where(s\"targetday in (\" + s\"date_sub('$targetDay',59),\" + s\"date_sub('$targetDay',49),\" + s\"date_sub('$targetDay',39),\" + s\"date_sub('$targetDay',29),\" + s\"date_sub('$targetDay',14),\" + s\"date_sub('$targetDay',6),\" + s\"date_sub('$targetDay',5),\" + s\"date_sub('$targetDay',4),\" + s\"date_sub('$targetDay',3),\" + s\"date_sub('$targetDay',2),\" + s\"date_sub('$targetDay',1),\" + s\"date_sub('$targetDay',0))\" ).show targetday 为 表的分区字段，date类型。 当 in 后面 的日期个数大于 10 时就会报错，小于等于 10 时都不会报错。奇怪的现象。。。 targetday 为 string 类型不会有此错误。 ","date":"2023-04-17","objectID":"/posts/spark-hive-partition-issue/:0:0","tags":["spark","大数据"],"title":"spark 读取 hive date 分区表 奇怪的报错","uri":"/posts/spark-hive-partition-issue/"},{"categories":["编程"],"content":"线上项目出现一个很奇怪的问题，数据经过Spark程序消费Kafka写入Kudu，出现Kudu Master连接超时，这个问题开始排查不出原因，有点头大，只能采用下下策，重启Spark程序，出现过几次后， 我记录了出现的时间，发现每次出现时间有个固定周期，一周，有规律就是最大的好消息，感觉离发现真相不远了，果然网上有这方面的问题讨论，虽说以前也去网上搜索过相关问题，毕竟Kudu相比于Hive、HBase还是小众了一点。。。是Kudu Java Client的问题，使用1.5以上版本就没问题了。 参考：关于kudu使用的一些问题及解决办法 ","date":"2023-04-17","objectID":"/posts/kudu-periodic-issues/:0:0","tags":["kudu","大数据"],"title":"Apache Kudu 写入数据定期出问题","uri":"/posts/kudu-periodic-issues/"},{"categories":["编程"],"content":"最近线上的spark项目日志文件急剧增加，磁盘顶不住了啊，解决日志文件问题，参考下面三篇文章，基本就可以搞明白了。 Spark日志过大导致磁盘溢出问题解决方案 Spark日志配置及问题排查方式。 Spark log4j 日志配置详解 以上内容转载自网络，如有侵权，请联系删除。 ","date":"2023-04-17","objectID":"/posts/spark-yarn-cluster-log4j/:0:0","tags":["spark","大数据"],"title":"spark yarn cluster模式下log4j日志的配置","uri":"/posts/spark-yarn-cluster-log4j/"},{"categories":["分享"],"content":"转载自网络，不保证可用性及是否拥有版权，仅供分享，如有法律纠纷，请联系删除。 国内网站 1、鸠摩搜书 网址：https://www.jiumodiary.com/ 一个强大的搜书神站，无论是什么类型的书籍，只要你知道书名，就可以轻松的搜到你想要书籍。页面简单明了，书籍种类繁多，格式多种多样，有mobi格式、pdf格式、word格式、txt格式等。关键是可以无限下载，无需注册登录。 2、周读 网址：http://www.ireadweek.com/index.php/Index/index.html 周读是一个提供优质的epud,mobi,pdf,txt电子书下载和分享的网站，帮助不知道读什么书的用户，选择值得读的好书籍，该网站提供多种书籍分类，涵盖了大多数的可读书籍。 3、我的小书屋 网址：http://mebook.cc/ 免费无广告、电子书的质量很好，图书都是经过站长精挑细选的。网站分为畅销小说、网络小说、合集资源、多看专区、杂志期刊、漫画、工具书、原版书籍、轻小说等模块。站长每天都会推荐更新几本书，书太多不知道读什么，就试试站长推荐的吧。 4、书格 网址： https://shuge.org/ 书格是一个自由开放的在线古籍图书馆，致力于开放式分享、介绍、推荐古本（四九以前的影像本）PDF；网站致力于为古籍的保护与数字化传播贡献。 5、西林街搜索 网址：http://www.xilinjie.com/ 专注于网盘、视频、文库（文档、古籍、专业书籍、电子书PDF、ePub、Mobi等格式）、学术（各种期刊、论文、学报等）和Mooc（在线课程、学习、视频教程）等资源的搜索。 6、好读 网址：http://haodoo.net 免费的线上繁体中文图书馆，可在线阅读及下载，可下载的格式有：updb，pdb，prc，epub。 7、图灵社区 网址：http://www.ituring.com.cn/ 图灵社区主要专注于科技书籍的出版，包括计算机、数学统计、科普等领域，提供免费和付费的电子书。用户可以使用网银或者支付宝的购买方式，大部分电子书同时提供了三种阅读方式：在线阅读、MOBI 推送、PDF 下载，部分书籍只提供其中部分格式。 8、盘搜 网址：http://www.pansoso.com/ 一个老牌网盘搜索工具，功能非常强大，界面非常简洁。每天都有更新，不同达人分享自己的“盘中资源”！盘搜不存储任何网盘内容，但是资源非常丰富，无论是工作还是学习都必备。 9、苦瓜书盘 网址：https://kgbook.com/ 苦瓜书盘,电子书分享的平台。适合电纸书阅读的6寸pdf及mobi格式电子书制作技术的网站。 10、新浪爱问共享资料 网址：http://ishare.iask.sina.com.cn/ 新浪爱问共享资料是新浪旗下的在线资料分享站，免费高速上传或下载各类资源，内容涉及教育资源、专业资料、IT的资料、娱乐生活、经济管理、办公文书、游戏资料等。如果你要寻找偏学术的资料，所有地方都找不到，可以来这里试试，一般为扫描版PDF。 11、E书联盟 网址：http://www.book118.com/ E书联盟是一个庞大的免费中文电子书下载站，提供各类电子书下载，其中含比较专业的电子图书，提供部分电子书在线阅读。 12、云海电子图书馆 网址：http://www.pdfbook.cn/ 云海电子图书馆是致力于pdf电子书的专业网站,提供各门类pdf电子书下载及高清pdf电子书。 13、万千集合站 网址：http://www.hejizhan.com/html/search 包含非常多的教材类相关电子书籍，搜索后直接显示下载链接。搜索结果基本涵盖了所有版本的电子教材、习题详解等。如果上大学时有这种资源，就省下好多买教材、习题解答的钱了。 14、蚂蚁搜书 网址：http://book.mybanshu.win/ IT书更多的资源网站，支持下载，不支持kindle推送。注册需要加微信索要邀请码，也是管理很严格了。 15、书语者 网址：https://book.shuyuzhe.com/ 书语者电子图书馆，一个搜书网站，让你快速找到想要的书籍！ 16、计算机书控 网址：http://bestcbooks.com/ 这个网站可谓是计算机专业的福音，这里包含许多优秀的计算机书籍下载。无论是英文原版还是中文这里都有。 17、影印古籍资料 网址：https://sou-yun.com/eBookIndex.aspx 提供7000+种古籍资料的网上阅读和PDF格式下载服务。 18、知轩藏书 网址：http://www.zxcs8.com/ 这是一个小说网站，在这里你可以找到你想要的小说。 19、国学网 地址：http://www.guoxue.com/ 一家在国学传播领域独具特色的文化创意企业，主要从事古籍数字化研究、网络文献检索开发和网站建设，是中国最大的专业古籍电子文献数据公司之一。 20、国家数字图书馆 网址： http://www.nlc.gov.cn/index.htm 大量在线资源、子数据库、可以在线阅读海量电子书，需要注册（实名注册、需要身份证号）。国图购买了大量资源，有账号，便可在线阅读或者下载，版权期限内的图书只能读前两章，民国图书和古籍则可阅读全部内容。 21、PDF之家 网址： http://www.pdfzj.com/ PDF之家，做中国最好的pdf资源站，致力最全最新的pdf杂志、期刊杂志、电子杂志、电子图书的免费分享和下载服务。 22、我爱读电子书 网址：http://www.woaidu.org/ 我爱读电子书是权威电子书搜索引擎，可以使用我爱读电子书快速的找到自己喜欢的电子书，也可以通过书榜了解最新最热门的书，随时随地畅想自己喜欢的书。 23、高清杂志网 网址：http://www.gqzzw.com/ 高清杂志网提供杂志国内热门原版高清电子杂志下载服务,目前有财经理财、电脑数码、故事传奇、婚姻家庭、健康养生、教育教学、经济法律、科技科普、旅游民俗、女性杂志等等。 24、超星电子书 网址：http://www.chaoxing.com 40万电子书，注册后可以在线或使用客户端阅读海量书籍。若在教育网中，推荐包库网址 ，可以将PDG格式电子书下载到本机上离线阅读。pdg如何转成pdf，请看百度经验 。该站共享资料页也有大量资源。 25、走读派 地址：http://zoudupai.com 瀑布流电子书网站，支持下载和推送，注册登陆后无其他限制，但是书不多，上传资源需要先申请。资源质量有待优化。其书评系统 #书瓦台# 形式接近微博。 26、掌上书苑 地址：https://www.cnepub.com/ Epub格式电子书下载站，下载和推送均为付费服务，但是可以免费在线阅读 27、众人搜索网 网址：http://dianzishu.renrensousuo.com/ 一个电子书搜索功能的网站，可以同时搜索各类电子书、电子小说等。 28、胖次搜索 网址：https://www.panc.cc/ 胖次的搜索结果需要配合繁体字才能有最佳效果，整体来说胖次还蛮好用的，连隐秘绝迹的讲座录音/笔记都有，这个必须给点个赞！界面做的也挺简洁，资源方面说实话也够看了。胖次可搜索影音视频、音乐歌曲、小说文档、程序App、图片壁纸、压缩文件BT种子等资源；胖次搜索如果是无效资源，会显示出来，这个很方便。 29、盘多多 网址：http://www.panduoduo.net/ 热门资源、专业资源方面出来的结果都很棒。office2016，各科目的文献资料都有，只要点击链接，就能进入详细页面进行下载，在页面下方，也列出了其他相关资源，全站无诱导下载的假按钮，真心不错。小问题：资源最丰富，但是偶尔不稳定。 30、去转盘网 网址：http://www.quzhuanpan.com/ 去转盘有着风景宜人的界面，是一个网盘搜索引擎，可以搜索bt和网盘资源还有资源分享等功能，搜索类型主要有影视、音乐、电子书、种子、软件等各种资源，搜索网盘为百度网盘。同时开发了PC端和app客户端，链接龙轩导航、哔哩哔哩、咪咕鱼等网站，功能十分强大。 31、麦库搜索 网址：http://huisou.me/ 麦库搜索界面相当简约，但不影响其强大的搜索功能。麦库是利用Google创建的一系列垂直搜索引擎，所以Google的一些使用技巧同样适合于麦库搜索。麦库数据来源：百度网盘，新浪微盘等。 32、特百度 网址：http://www.tebaidu.com/ 特百度云提供百度云旗下的百度网盘搜索下载百度网盘的资源，本站也支持百度网盘登陆，百度网盘是目前受欢迎的T级超大免费网盘，注册用户过亿。 33、史莱姆搜索 网址：http://www.slimego.cn/ 最丰富的学习资料库,收集整理了大量免费资源,教学资源,百度云资源,网盘资源。包含主流的资源搜索外,还能找到各行业细分的学习资源。 国外网站 34、Library Genesis 网址：http://gen.lib.rus.ec/ 这个界面简洁明了，输入你想要找的关键词或者全名，就可以搜到该领域的一些电子书籍。一些科研的电子书，还有科研论文、小说、喜剧、行业标准、杂志啥的都可以搜搜试试。从这个网站搜索可以搜出一本书的好多版本，大家根据自己的需求下载。 35、Project Gutenberg 网址：http://www.gutenberg.org/ 它是国外一个知名的电子书免费分享网站，旨在基于互联网，大量提供版权过期而进入公有领域书籍的一项协作计划，","date":"2023-04-17","objectID":"/posts/book-search-website/:0:0","tags":["杂项"],"title":"五十个电子书搜索网","uri":"/posts/book-search-website/"},{"categories":["编程"],"content":"配置好下面三个属性。 \u003c!-- 超过时间限制是否回收 --\u003e \u003cproperty name=\"removeAbandoned\" value=\"true\" /\u003e \u003c!-- 超时时间；单位为秒。180秒=3分钟 --\u003e \u003cproperty name=\"removeAbandonedTimeout\" value=\"180\" /\u003e \u003c!-- 关闭abanded连接时输出错误日志 --\u003e \u003cproperty name=\"logAbandoned\" value=\"true\" /\u003e 查看日志文件。 2019-04-17 17:10:05:140 - ERROR [Druid-ConnectionPool-Destroy-1559154670] - com.alibaba.druid.pool.DruidDataSource com.alibaba.druid.pool.DruidDataSource.removeAbandoned:2666 abandon connection, owner thread: http-nio-8085-exec-1, connected at : 1555491605100, open stackTrace at java.lang.Thread.getStackTrace(Thread.java:1559) at com.alibaba.druid.pool.DruidDataSource.getConnectionDirect(DruidDataSource.java:1313) at com.alibaba.druid.pool.DruidDataSource.getConnection(DruidDataSource.java:1235) at com.alibaba.druid.pool.DruidDataSource.getConnection(DruidDataSource.java:1225) at com.xxxx.xx.common.util.JdbcUtil.linkTrace(JdbcUtil.java:39) at com.xxxx.xx.xxxxx.service.impl.DashboardServiceImpl.getExpenTrend(DashboardServiceImpl.java:263) at com.xxxx.xx.xxxxx.service.impl.DashboardServiceImpl.getBossData(DashboardServiceImpl.java:58) at com.xxxx.xx.xxxxx.controller.DashboardController.getBossData(DashboardController.java:43) 日志里查找removeAbandoned关键字，找到自己哪个方法获取连接后没有关闭资源，排查原因。 ","date":"2023-04-17","objectID":"/posts/druid-connect-not-free/:0:0","tags":["java"],"title":"阿里Druid连接池连接不释放、连接泄漏排查","uri":"/posts/druid-connect-not-free/"},{"categories":["编程"],"content":"最近用到阿里云oss，有阿里云服务器，通过代理内网访问可以实现免除OSS流量费，查到很多nginx反向代理的教程，但是纯java实现没有找到，感觉可以试一试。 （一） 首先要解决反向代理的问题，搜到org.mitre.dsmiley.httpproxy.ProxyServlet可解决。 \u003cdependency\u003e \u003cgroupId\u003eorg.mitre.dsmiley.httpproxy\u003c/groupId\u003e \u003cartifactId\u003esmiley-http-proxy-servlet\u003c/artifactId\u003e \u003cversion\u003e1.11\u003c/version\u003e \u003c/dependency\u003e @Configuration public class MyConfig { @Bean public ServletRegistrationBean servletRegistrationAliBean01() { ServletRegistrationBean oss = new ServletRegistrationBean(new ProxyServlet(), \"/ali/bucket_name/*\");//修改为自己的bucket_name oss.setName(\"bucket_name\"); oss.addInitParameter(\"targetUri\", \"http://bucket_name.oss-cn-beijing.aliyuncs.com\");//本地测试环境用外网地址，线上改成内网访问的地址 oss.addInitParameter(ProxyServlet.P_LOG, \"true\"); return oss; } } （二） Controller配置如下，转发到自己配置的反向代理 @RestController @RefreshScope @RequestMapping(\"/api/es\") public class TestController { @RequestMapping(\"/static/**\") public void testVideoOne(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException, URISyntaxException { String resourcePath = request.getRequestURI().replace(\"/api/es/static\",\"\"); HeaderUtil.setHeaderOwn(request,resourcePath); request.getRequestDispatcher(\"/ali\" + resourcePath).forward(request, response); } } （三） 其中为request设置Header以认证OSS访问的HeaderUtil如下，其中有OSS Authorization计算的java实现，用java的反射在request中添加Header信息。 import org.apache.commons.codec.binary.Base64; import org.apache.tomcat.util.http.MimeHeaders; import javax.crypto.Mac; import javax.crypto.spec.SecretKeySpec; import javax.servlet.http.HttpServletRequest; import java.lang.reflect.Field; import java.net.URISyntaxException; import java.nio.charset.Charset; import java.security.InvalidKeyException; import java.security.NoSuchAlgorithmException; import java.text.SimpleDateFormat; import java.util.Date; import java.util.Locale; import java.util.TimeZone; public class HeaderUtil { /** * 给request加入Header信息，验证OSS访问权限 * @param request * @throws URISyntaxException */ public static void setHeaderOwn(HttpServletRequest request,String resourcePath) throws URISyntaxException { SimpleDateFormat sdf = new SimpleDateFormat(\"EEE, dd MMM yyyy HH:mm:ss 'GMT'\", Locale.US); sdf.setTimeZone(TimeZone.getTimeZone(\"GMT\")); String date_GMT = sdf.format(new Date()); String data = request.getMethod() + \"\\n\" + \"\\n\" + \"\\n\" + date_GMT + \"\\n\" + resourcePath; String signature = genHMAC(\"AccessKeySecret\", data);//AccessKeySecret改成自己的 reflectSetParam(request, \"Authorization\", \"OSS AccessKeyId:\" + signature);//AccessKeyId改成自己的 reflectSetParam(request, \"Date\", date_GMT); } /** * 反射修改header信息，key-value键值对加入到header中 * * @param request * @param key * @param value */ private static void reflectSetParam(HttpServletRequest request, String key, String value) { Class\u003c? extends HttpServletRequest\u003e requestClass = request.getClass(); // System.out.println(\"request实现类=\"+requestClass.getName()); try { Field request1 = requestClass.getDeclaredField(\"request\"); request1.setAccessible(true); Object o = request1.get(request); Field coyoteRequest = o.getClass().getDeclaredField(\"coyoteRequest\"); coyoteRequest.setAccessible(true); Object o1 = coyoteRequest.get(o); // System.out.println(\"coyoteRequest实现类=\"+o1.getClass().getName()); Field headers = o1.getClass().getDeclaredField(\"headers\"); headers.setAccessible(true); MimeHeaders o2 = (MimeHeaders) headers.get(o1); o2.addValue(key).setString(value); } catch (Exception e) { e.printStackTrace(); } } /** * HmacSHA1 Base64加密 * @param key * @param data * @return */ public static String genHMAC(String key, String data) { byte[] result = null; try { //根据给定的字节数组构造一个密钥,第二参数指定一个密钥算法的名称 SecretKeySpec signinKey = new SecretKeySpec(key.getBytes(), \"HmacSHA1\"); //生成一个指定 Mac 算法 的 Mac 对象 Mac mac = Mac.getInstance(\"HmacSHA1\"); //用给定密钥初始化 Mac 对象 mac.init(signinKey); //完成 Mac 操作 byte[] rawHmac = mac.doFinal(data.getBytes(Charset.forName(\"UTF-8\"))); result = Base64.encodeBase64(rawHmac); } catch (NoSuchAlgorithmException e) { System.err.println(e.getMessage()); } catch (InvalidKeyException e) { System","date":"2023-04-17","objectID":"/posts/spring-proxy-oss/:0:0","tags":["java","oss"],"title":"spring cloud java反向代理访问阿里云OSS私有资源","uri":"/posts/spring-proxy-oss/"},{"categories":null,"content":"\u003cno value\u003e's friends","date":"2023-04-17","objectID":"/friends/","tags":null,"title":"友情链接","uri":"/friends/"},{"categories":null,"content":"基本信息 - nickname: Lruihao avatar: https://lruihao.cn/images/avatar.jpg url: https://lruihao.cn description: Lruihao's Note ","date":"2023-04-17","objectID":"/friends/:1:0","tags":null,"title":"友情链接","uri":"/friends/"},{"categories":null,"content":"友情提醒 Notice 互换友链请按以上格式在评论留言。（仅限个人非商业博客 / 网站） 提醒：网站失效、停止维护、内容不当都可能被取消连接！ 那些不尊重他人劳动成果，转载不加出处的，或恶意行为的网站，还请您不要来进行交换了。 ","date":"2023-04-17","objectID":"/friends/:2:0","tags":null,"title":"友情链接","uri":"/friends/"}]