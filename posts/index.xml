<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>所有文章 - PME-Blog</title><link>https://hpk.me/posts/</link><description>PME-Blog 难得自在</description><generator>Hugo 0.147.9 &amp; FixIt v0.3.21-b2e6f70a</generator><language>zh-CN</language><lastBuildDate>Thu, 26 Jun 2025 01:25:29 +0800</lastBuildDate><atom:link href="https://hpk.me/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>Spark通过Jdbc读取Doris varchar类型长度异常</title><link>https://hpk.me/posts/spark-jdbc-doris-dialect/</link><pubDate>Wed, 25 Jun 2025 00:27:44 +0800</pubDate><author>hpkaiq@qq.com (hpkaiq)</author><guid>https://hpk.me/posts/spark-jdbc-doris-dialect/</guid><category domain="https://hpk.me/categories/%E7%BC%96%E7%A8%8B/">编程</category><description>&lt;p>spark内通过jdbc方式读取阿里云selectdb（doris）数据，varchar类型长度不足85时总是自动填充空格至85长度，例如表name varchar(20) 字段，name值为 &amp;lsquo;xxx&amp;rsquo;，spark内通过jdbc方式读出来显示&amp;rsquo;xxx&amp;rsquo;后补空格至总长度85位，在spark里length(name)也是85。&lt;/p></description></item><item><title>hive 日期维度表</title><link>https://hpk.me/posts/hive-dim-date/</link><pubDate>Thu, 12 Dec 2024 11:38:59 +0800</pubDate><author>hpkaiq@qq.com (hpkaiq)</author><guid>https://hpk.me/posts/hive-dim-date/</guid><category domain="https://hpk.me/categories/%E7%BC%96%E7%A8%8B/">编程</category><description>&lt;p>hive日期维度表，常用的字段基本都有。&lt;/p></description></item><item><title>一些免费的大模型资源</title><link>https://hpk.me/posts/about_chatgpt/</link><pubDate>Tue, 12 Mar 2024 11:22:13 +0800</pubDate><author>hpkaiq@qq.com (hpkaiq)</author><guid>https://hpk.me/posts/about_chatgpt/</guid><category domain="https://hpk.me/categories/chatgpt/">Chatgpt</category><description>&lt;p>一些免费的大模型资源，可生成兼容Openai api 接口的资源。&lt;/p></description></item><item><title>Openwrt下使用TOTP动态验证码连接Openvpn</title><link>https://hpk.me/posts/openwrt-openvpn/</link><pubDate>Thu, 20 Jul 2023 12:24:22 +0800</pubDate><author>hpkaiq@qq.com (hpkaiq)</author><guid>https://hpk.me/posts/openwrt-openvpn/</guid><category domain="https://hpk.me/categories/openwrt/">Openwrt</category><description>&lt;h2 class="heading-element" id="背景">&lt;span>背景&lt;/span>
 &lt;a href="#%e8%83%8c%e6%99%af" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;p>在家办公需要使用 openvpn 连接公司内网，而且密码使用 固定密码 +TOTP 验证码 的形式，每次都需要打开电脑的openvpn客户端，打开验证码客户端复制验证码，连接，只能说繁琐。希望通过家里的软路由openwrt的openvpn客户端来实现上述操作的一键连接，实现家里每个终端设备都能连接公司内网。&lt;/p></description></item><item><title>m3u8格式直播地址文件使用php在线转换成txt格式</title><link>https://hpk.me/posts/m3u8-convert-to-txt/</link><pubDate>Mon, 29 May 2023 11:19:42 +0800</pubDate><author>hpkaiq@qq.com (hpkaiq)</author><guid>https://hpk.me/posts/m3u8-convert-to-txt/</guid><category domain="https://hpk.me/categories/php/">Php</category><description>&lt;p>一些m3u或者m3u8格式的直播地址文件在一些软件中并不支持，在chatgpt的帮助下，使用php实现了在线转换成txt格式，需要能运行php的服务器。&lt;/p></description></item><item><title>Cloudflare优选ip并使用dnspodcn api设置解析</title><link>https://hpk.me/posts/cloudflare-betterip-dnspodcn/</link><pubDate>Mon, 29 May 2023 10:45:18 +0800</pubDate><author>hpkaiq@qq.com (hpkaiq)</author><guid>https://hpk.me/posts/cloudflare-betterip-dnspodcn/</guid><category domain="https://hpk.me/categories/cloudflare/">Cloudflare</category><description>&lt;p>Cloudflare优选ip并使用dnspodcn api设置解析，实现在本地网络环境下nas或者其他服务器上优选IP，并自动解析到自己的dnspodcn域名，实现速度最优。&lt;/p></description></item><item><title>spark 实现 mysql upsert，可忽略null值</title><link>https://hpk.me/posts/spark-mysql-upsert/</link><pubDate>Tue, 18 Apr 2023 14:23:06 +0800</pubDate><author>hpkaiq@qq.com (hpkaiq)</author><guid>https://hpk.me/posts/spark-mysql-upsert/</guid><category domain="https://hpk.me/categories/%E7%BC%96%E7%A8%8B/">编程</category><description>&lt;p>&lt;strong>实现 spark dataframe/dataset 根据mysql表唯一键实现有则更新，无则插入功能。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>2024.07.25更新：新增特性，忽略值为null的列，即当df里列值为null时，不更新mysql表数据，保留表原有的值。&lt;/strong>&lt;/p></description></item><item><title>java 频次控制</title><link>https://hpk.me/posts/java-rate-limit-google-guava/</link><pubDate>Tue, 18 Apr 2023 14:21:15 +0800</pubDate><author>hpkaiq@qq.com (hpkaiq)</author><guid>https://hpk.me/posts/java-rate-limit-google-guava/</guid><category domain="https://hpk.me/categories/%E7%BC%96%E7%A8%8B/">编程</category><description>&lt;p>访问某接口拉取数据，接口需要频次控制，经调研，&lt;code>com.google.common.util.concurrent.RateLimiter&lt;/code>可轻易实现。&lt;/p></description></item><item><title>python3 easyocr 简单使用识别参数</title><link>https://hpk.me/posts/python3-easyocr-simple/</link><pubDate>Tue, 18 Apr 2023 14:18:37 +0800</pubDate><author>hpkaiq@qq.com (hpkaiq)</author><guid>https://hpk.me/posts/python3-easyocr-simple/</guid><category domain="https://hpk.me/categories/%E7%BC%96%E7%A8%8B/">编程</category><description>&lt;p>python3 easyocr 学习，测试。&lt;/p></description></item><item><title>presto 自定义函数简述</title><link>https://hpk.me/posts/presto-udf-simple/</link><pubDate>Tue, 18 Apr 2023 14:16:42 +0800</pubDate><author>hpkaiq@qq.com (hpkaiq)</author><guid>https://hpk.me/posts/presto-udf-simple/</guid><category domain="https://hpk.me/categories/%E7%BC%96%E7%A8%8B/">编程</category><description>&lt;p>presto自带unbase64函数某些时候会报错，所以想要自定义一个unbase64函数。&lt;/p></description></item><item><title>python3 pandas 实现mysql upsert操作（唯一键更新）</title><link>https://hpk.me/posts/python3-pandas-mysql-upsert/</link><pubDate>Tue, 18 Apr 2023 14:15:04 +0800</pubDate><author>hpkaiq@qq.com (hpkaiq)</author><guid>https://hpk.me/posts/python3-pandas-mysql-upsert/</guid><category domain="https://hpk.me/categories/%E7%BC%96%E7%A8%8B/">编程</category><description>&lt;p>python3 pandas 实现mysql upsert操作，基于唯一键有则更新，无则插入。&lt;/p></description></item><item><title>sqoop mysql update AUTO_INCREMENT 自增主键重复增长问题</title><link>https://hpk.me/posts/sqoop-mysql-update-auto-increment/</link><pubDate>Tue, 18 Apr 2023 14:11:19 +0800</pubDate><author>hpkaiq@qq.com (hpkaiq)</author><guid>https://hpk.me/posts/sqoop-mysql-update-auto-increment/</guid><category domain="https://hpk.me/categories/%E7%BC%96%E7%A8%8B/">编程</category><description>sqoop mysql update AUTO_INCREMENT 自增主键重复增长问题</description></item><item><title>脚本执行spark-shell scala文件退出</title><link>https://hpk.me/posts/bash-spark-shell-scala/</link><pubDate>Tue, 18 Apr 2023 14:09:10 +0800</pubDate><author>hpkaiq@qq.com (hpkaiq)</author><guid>https://hpk.me/posts/bash-spark-shell-scala/</guid><category domain="https://hpk.me/categories/%E7%BC%96%E7%A8%8B/">编程</category><description>脚本执行spark-shell scala文件退出</description></item><item><title>kudu-spark KuduContext java.io.InvalidClassException 解决</title><link>https://hpk.me/posts/kudu-spark-invalid-class-exception/</link><pubDate>Tue, 18 Apr 2023 13:56:34 +0800</pubDate><author>hpkaiq@qq.com (hpkaiq)</author><guid>https://hpk.me/posts/kudu-spark-invalid-class-exception/</guid><category domain="https://hpk.me/categories/%E7%BC%96%E7%A8%8B/">编程</category><description>&lt;p>背景：
线上kudu 集群版本为1.11.0版本, spark 使用 kudu-spark2_2.11-1.7.0.jar，
为了使用新版本中的
&lt;code>val wo = new KuduWriteOptions(ignoreNull = true)&lt;/code>
特性，升级至 kudu-spark2_2.11-1.11.0.jar 版本，但是报错。&lt;/p></description></item><item><title>phoenix-client-4.14.1-HBase-1.4.jar jar包冲突解决</title><link>https://hpk.me/posts/phoenix-client-hbase-jar/</link><pubDate>Tue, 18 Apr 2023 13:49:28 +0800</pubDate><guid>https://hpk.me/posts/phoenix-client-hbase-jar/</guid><category domain="https://hpk.me/categories/%E7%BC%96%E7%A8%8B/">编程</category><description>&lt;p>项目用到phoenix，使用了这个jar包phoenix-client-4.14.1-HBase-1.4.jar，这个jar包导致的jar包冲突很多，一番摸索，解决了，解决如下。&lt;/p></description></item><item><title>spark 读取 hive date 分区表 奇怪的报错</title><link>https://hpk.me/posts/spark-hive-partition-issue/</link><pubDate>Mon, 17 Apr 2023 21:29:55 +0800</pubDate><guid>https://hpk.me/posts/spark-hive-partition-issue/</guid><category domain="https://hpk.me/categories/%E7%BC%96%E7%A8%8B/">编程</category><description>spark 读取 hive date 分区表 奇怪的报错</description></item><item><title>Apache Kudu 写入数据定期出问题</title><link>https://hpk.me/posts/kudu-periodic-issues/</link><pubDate>Mon, 17 Apr 2023 21:23:09 +0800</pubDate><guid>https://hpk.me/posts/kudu-periodic-issues/</guid><category domain="https://hpk.me/categories/%E7%BC%96%E7%A8%8B/">编程</category><description>Apache Kudu 写入数据定期出问题</description></item><item><title>spark yarn cluster模式下log4j日志的配置</title><link>https://hpk.me/posts/spark-yarn-cluster-log4j/</link><pubDate>Mon, 17 Apr 2023 21:19:43 +0800</pubDate><guid>https://hpk.me/posts/spark-yarn-cluster-log4j/</guid><category domain="https://hpk.me/categories/%E7%BC%96%E7%A8%8B/">编程</category><description>spark yarn cluster模式下log4j日志的配置</description></item><item><title>五十个电子书搜索网</title><link>https://hpk.me/posts/book-search-website/</link><pubDate>Mon, 17 Apr 2023 21:15:15 +0800</pubDate><guid>https://hpk.me/posts/book-search-website/</guid><category domain="https://hpk.me/categories/%E5%88%86%E4%BA%AB/">分享</category><description>五十个电子书搜索网</description></item><item><title>阿里Druid连接池连接不释放、连接泄漏排查</title><link>https://hpk.me/posts/druid-connect-not-free/</link><pubDate>Mon, 17 Apr 2023 21:12:25 +0800</pubDate><guid>https://hpk.me/posts/druid-connect-not-free/</guid><category domain="https://hpk.me/categories/%E7%BC%96%E7%A8%8B/">编程</category><description>阿里Druid连接池连接不释放、连接泄漏排查</description></item><item><title>spring cloud java反向代理访问阿里云OSS私有资源</title><link>https://hpk.me/posts/spring-proxy-oss/</link><pubDate>Mon, 17 Apr 2023 20:43:10 +0800</pubDate><guid>https://hpk.me/posts/spring-proxy-oss/</guid><category domain="https://hpk.me/categories/%E7%BC%96%E7%A8%8B/">编程</category><description>&lt;p>最近用到阿里云oss，有阿里云服务器，通过代理内网访问可以实现免除OSS流量费，查到很多nginx反向代理的教程，但是纯java实现没有找到，感觉可以试一试。&lt;/p></description></item></channel></rss>